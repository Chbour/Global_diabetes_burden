{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path as op\n",
    "import unicodedata\n",
    "import sys\n",
    "from gensim.models import FastText\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_predict, StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "\n",
    "\n",
    "basename = r\"..\" #path to basename\n",
    "path_utils = op.join(basename , \"utils\")\n",
    "sys.path.insert(0, path_utils)\n",
    "\n",
    "from sys_utils import load_library\n",
    "load_library(op.join(basename, 'readWrite'))\n",
    "from readWrite import readFile\n",
    "from sys_utils import load_library\n",
    "from tweet_utils import *\n",
    "\n",
    "from preprocess import Preprocess\n",
    "prep = Preprocess()\n",
    "\n",
    "model_we = #load word embedding model\n",
    "\n",
    "trainingData = #import labeled data for training\n",
    "\n",
    "\n",
    "print(trainingData.dtypes)\n",
    "print(trainingData.shape)\n",
    "trainingData.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_names = pd.read_csv(r\"\\data\\BabyNames_2018_US_SSA.txt\", \n",
    "                           sep=\",\", header=None, names=[\"name\", \"gender\", \"OccurencesName\"])\n",
    "\n",
    "print(gender_names.shape)\n",
    "gender_names.name = gender_names.name.map(lambda name: name.lower())\n",
    "\n",
    "\n",
    "def choose_most_occuring_name(df):\n",
    "    if df.shape[0] == 1: return df\n",
    "    else:\n",
    "        maxEl = df.OccurencesName.max(axis=0)\n",
    "        return df[df[\"OccurencesName\"] == maxEl]\n",
    "\n",
    "# in duplicate cases (Name exist for male and female) take only the one that occurs more often\n",
    "gender_names = gender_names.groupby(\"name\",as_index=False).apply(choose_most_occuring_name)\n",
    "gender_names.reset_index(drop=True, inplace=True)\n",
    "print(gender_names.shape)\n",
    "\n",
    "gender_names.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"\", usecols=[\"id\", \"text\", \"user_description\", \"user_name\", \"user_screen_name\"]).sample(n=12000, random_state=1) #import data to classify\n",
    "\n",
    "def get_sex(name):\n",
    "    try:\n",
    "        firstName = unicodedata.normalize('NFKD', name).encode('ascii', 'ignore').decode('utf-8', 'ignore').split(\" \")[0].replace(\" \", \"\")\n",
    "        return firstName.lower()\n",
    "    except:\n",
    "        return None \n",
    "\n",
    "df[\"firstName\"] = df.user_name.map(get_sex)\n",
    "\n",
    "print(df.shape)\n",
    "new = pd.merge(df, gender_names, how='inner', left_on=\"firstName\", right_on=\"name\")\n",
    "new = new[new.OccurencesName > 500] # take only examples of names that occur often\n",
    "new = new[[\"text\", \"user_description\", \"user_name\", \"gender\"]]\n",
    "print(new.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData[\"History_Sex\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    tweet = prep.replace_hashtags_URL_USER(tweet, mode_URL=\"replace\", mode_Mentions=\"replace\")\n",
    "    tweet = prep.tokenize(tweet)\n",
    "\n",
    "    return tweet\n",
    "\n",
    "def labelEncode(sex):\n",
    "    if sex == \"M\": return(0) \n",
    "    elif sex == \"F\": return(1)\n",
    "    elif sex == \"U\": return(2)\n",
    "    else: return(2)\n",
    "\n",
    "\n",
    "def create_history_sex_column(row):\n",
    "    if row[\"History_Sex\"] == \"M\": return \"M\"\n",
    "    elif row[\"History_Sex\"] == \"F\": return \"F\"\n",
    "    elif row[\"History_Sex\"] == \"U\": return \"U\"\n",
    "    elif pd.isnull(row[\"History_Sex\"]): return row[\"Sexe\"]\n",
    "    else: print(\"ERROR: Should not occur:  \", row[\"Sexe\"], \";;;\", row[\"History_Sex\"])\n",
    "\n",
    "trainingData['history_sex_total'] = trainingData.apply (lambda row: create_history_sex_column(row), axis=1)\n",
    "\n",
    "sex = trainingData.Sexe\n",
    "history_sex = trainingData.history_sex_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPEND Tweets from onw tweet database tweets which matched some gender names\n",
    "print(\"trainingData.shape:\", trainingData.shape)\n",
    "print(\"Own data.shape:\", new.shape)\n",
    "\n",
    "label = \"history_sex_total\"\n",
    "data_pd = trainingData[[\"text\", \"user_description\", \"user_name\", label]]\n",
    "new[label] = new[\"gender\"]\n",
    "del new[\"gender\"]\n",
    "\n",
    "data_pd = data_pd.append(new, ignore_index=True).sample(frac=1.0) # append dataframes and sample\n",
    "print(data_pd[label].value_counts())\n",
    "data_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Username, user_description and text to vector representation\n",
    "data_pd.text = data_pd.text.map(lambda tweet: tweet_vectorizer(preprocess_tweet(tweet), model_we))\n",
    "data_pd[\"temp_userDesc\"] = data_pd.user_description.map(lambda userDesc: 0 \n",
    "                                                if isinstance(userDesc, float) or userDesc == \" \" or userDesc == None\n",
    "                                                else 1)\n",
    "\n",
    "data_pd.user_description = data_pd.user_description.map(lambda userDesc: np.zeros((model_we.vector_size, )) \n",
    "                                                if isinstance(userDesc, float) or userDesc == \" \" or userDesc == None\n",
    "                                                else tweet_vectorizer(preprocess_tweet(userDesc), model_we))\n",
    "\n",
    "def userName_to_vec(name):\n",
    "    \"\"\" Username to vector representation if possible, otherwise 0-vector \"\"\"\n",
    "    try:\n",
    "        firstName = unicodedata.normalize('NFKD', name).encode('ascii', 'ignore').decode('utf-8', 'ignore').split(\" \")[0].replace(\" \", \"\")\n",
    "        vec = model_we[firstName]\n",
    "    except:\n",
    "        vec = np.zeros((model_we.vector_size, ))\n",
    "    return vec\n",
    "\n",
    "def TEMP_userName_to_vec(name):\n",
    "    try:\n",
    "        firstName = unicodedata.normalize('NFKD', name).encode('ascii', 'ignore').decode('utf-8', 'ignore').split(\" \")[0].replace(\" \", \"\")\n",
    "        vec = model_we[firstName]\n",
    "        return 1\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "data_pd['temp_user_name'] = data_pd.user_name.map(lambda name: TEMP_userName_to_vec(name))\n",
    "data_pd.user_name = data_pd.user_name.map(lambda name: userName_to_vec(name))\n",
    "data_pd[label] = data_pd[label].map(labelEncode)\n",
    "\n",
    "\n",
    "# remove the tweets that are empty because there is no word embedding\n",
    "data_pd = data_pd[data_pd[\"text\"].apply(lambda x: len(x)>0) ]\n",
    "print(data_pd.shape)\n",
    "\n",
    "data_pd.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# helper functions for machine learning pipeline\n",
    "class ItemSelect(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data):\n",
    "        return np.asarray(data[self.key].values.tolist())\n",
    "    \n",
    "    \n",
    "class Debug(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, message=\"\"):\n",
    "        self.message = message\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        print(self.message)\n",
    "        print(\"type:\", type(X), \"len:\", X.shape)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose training algorithm:\n",
    "#---------------------------------------------------------------------------\n",
    "modelAlgo = \"SVC\"\n",
    "\n",
    "if modelAlgo == \"MultinomialNB\":\n",
    "    model = MultinomialNB(random_state=0)\n",
    "elif modelAlgo == \"SVC\":\n",
    "    model = SVC(random_state=0)\n",
    "elif modelAlgo == \"logReg\":\n",
    "    model = LogisticRegression(random_state=0)\n",
    "elif modelAlgo == \"RandomForest\" :\n",
    "    model = RandomForestClassifier(random_state=0)\n",
    "elif modelAlgo == \"XGBoost\" :\n",
    "    model = XGBClassifier(random_state=0)\n",
    "elif modelAlgo == \"MLP\" :\n",
    "    model = MLPClassifier(early_stopping=True, batch_size=32, random_state=0)\n",
    "\n",
    "# use Pipeline from imblearn package to be able to use SMOTE oversampling    \n",
    "from imblearn.pipeline import Pipeline    \n",
    "pipeline  = Pipeline([\n",
    "                ('union', FeatureUnion(\n",
    "                            transformer_list = [\n",
    "                                ('tweet', Pipeline([\n",
    "                                    ('tweetsSelector', ItemSelect(key='text')),\n",
    "                                ])),\n",
    "                                ('userDesc', Pipeline([\n",
    "                                    ('userDescSelector', ItemSelect(key='user_description'))\n",
    "                                ])),\n",
    "                                ('userName', Pipeline([\n",
    "                                    ('userNameSelector', ItemSelect(key='user_name'))\n",
    "                                ]))  \n",
    "                            ],\n",
    "                )),\n",
    "                ('smote', SMOTE(random_state=12, sampling_strategy=\"auto\",  n_jobs=-1)), #, ratio = 1.0 \n",
    "                ('model', model),\n",
    "            ])\n",
    "\n",
    "\n",
    "# parameter grid for grid search by using fastText embeddings\n",
    "parameters = {\n",
    "                'union__transformer_weights' : [\n",
    "#                                                {\"tweet\": 1, \"userDesc\":1, \"userName\":1},\n",
    "#                                                {\"tweet\": 1, \"userDesc\":1, \"userName\":0.8}, \n",
    "#                                                {\"tweet\": 1, \"userDesc\":1, \"userName\":0.5}\n",
    "#                                                {\"tweet\": 1, \"userDesc\":1, \"userName\":0.5},\n",
    "#                                                {\"tweet\": 1, \"userDesc\":1, \"userName\":0.4},\n",
    "#                                                {\"tweet\": 1, \"userDesc\":0.8, \"userName\":0.4},\n",
    "#                                                {\"tweet\": 1, \"userDesc\":0.8, \"userName\":0.5},\n",
    "#                                                {\"tweet\": 1, \"userDesc\":0.8, \"userName\":0.6},\n",
    "#                                                {\"tweet\": 1, \"userDesc\":0.7, \"userName\":0.5},\n",
    "#                                                {\"tweet\": 1, \"userDesc\":0.9, \"userName\":0.5},\n",
    "#                                                ],\n",
    "#                                               [\n",
    "#                                                {\"tweet\": 1, \"userDesc\":1, \"userName\":1}, \n",
    "#                                                {\"tweet\": 1, \"userDesc\":0.3, \"userName\":0.3}, \n",
    "#                                                {\"tweet\": 1, \"userDesc\":0.5, \"userName\":0.5},\n",
    "#                                                {\"tweet\": 1, \"userDesc\":0.2, \"userName\":0.5},\n",
    "#                                                {\"tweet\": 1, \"userDesc\":0.7, \"userName\":0.7},\n",
    "#                                                {\"tweet\": 1, \"userDesc\":0.8, \"userName\":0.5},\n",
    "#                                                {\"tweet\": 1, \"userDesc\":0.8, \"userName\":0.8}, \n",
    "#                                                {\"tweet\": 1, \"userDesc\":0.8, \"userName\":0.8},\n",
    "#                                                {\"tweet\": 1, \"userDesc\":0.5, \"userName\":0.8},\n",
    "#                                                {'tweet': 1, 'userDesc': 0.8, 'userName': 1},\n",
    "#                                                {\"tweet\": 1, \"userDesc\":0, \"userName\":1},\n",
    "#                                                {\"tweet\": 0, \"userDesc\":0, \"userName\":1}\n",
    "                                                ],\n",
    "\n",
    "#                'smote__k_neighbors' : [4],\n",
    "\n",
    "#               # param for SVC\n",
    "#               'model__kernel' : [\"linear\"],#[\"linear\", \"poly\", \"rbf\"],\n",
    "#               'model__C' : [0.1,],\n",
    "#               'model__tol' : [1e-3],\n",
    "#               'model__class_weight' : [\"balanced\", {1:0.5}, {1:1}, {1:1.5}],\n",
    "#\n",
    "#               # param for RandomForestClassifier\n",
    "#               'model__n_estimators' : [50, 100, 150],\n",
    "#               'model__criterion' : ['gini', 'entropy'],\n",
    "#               'model__max_features' : ['auto', 'log2'],\n",
    "#               'model__max_depth' : [ 5, 10, 20, 30]\n",
    "#\n",
    "#               # param for XGBoost Best: 0.910828 using {'model__learning_rate': 0.05, 'model__reg_alpha': 0, 'model__max_depth': 3, 'model__reg_lambda': 1.5, 'model__n_estimators': 300}\n",
    "#               'model__max_depth' : [3],#[3,4],\n",
    "#              'model__learning_rate' : [0.001, 0.01, 0.1],\n",
    "#               'model__booster' : [\"gbtree\", \"gblinear\", \"dart\"], # [\"gblinear\"], #\n",
    "#               'model__gamma' : [0, 0.01],\n",
    "#               'model__n_estimators' : [100, 150],\n",
    "#               'model__reg_alpha' : [0, 0.1],\n",
    "#               'model__reg_lambda' : [0.5, 1.0]\n",
    "\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(sex):\n",
    "    # M = 0, F = 1, U = 2\n",
    "    if sex == 0: return(-1) \n",
    "    else: return(1)\n",
    "\n",
    "print(\"data:\", data_pd.shape, type(data_pd))\n",
    "X = data_pd[[\"text\", \"user_description\", \"user_name\"]]\n",
    "y = data_pd[label]\n",
    "\n",
    "#temp = data_pd.loc[data_pd[label] != 2]\n",
    "#print(\"Temp:\", temp.shape, type(temp))\n",
    "#X = temp[[\"text\", \"user_description\", \"user_name\"]]\n",
    "#y = temp[label]#.map(label_encode)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) \n",
    "print(\"X_train:\", X_train.shape, type(X_train))\n",
    "print(\"X_test:\", X_test.shape, type(X_test))\n",
    "print(\"y_train:\", y_train.shape, type(y_train))\n",
    "print(\"y_test:\", y_test.shape, type(y_test))\n",
    "\n",
    "\n",
    "X_train_pd = pd.DataFrame(X_train, columns=[\"text\", \"user_description\", \"user_name\"])\n",
    "X_test_pd = pd.DataFrame(X_test, columns=[\"text\", \"user_description\", \"user_name\"])\n",
    "\n",
    "print(y_train.unique())\n",
    "print(\"Start Grid search...\")\n",
    "#from sklearn.metrics import precision_score, roc_auc_score, make_scorer\n",
    "grid = GridSearchCV(pipeline, parameters, cv=5, n_jobs=-2, verbose=1)#, scoring=prec_scorer)#, scoring=\"roc_auc\")\n",
    "grid.fit(X_train_pd, y_train)\n",
    "print(\"\\nBest: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
    "\n",
    "y_pred = grid.best_estimator_.predict(X_test_pd)\n",
    "#print(\"F1-Score:\", f1_score(y_test, y_pred))\n",
    "#print(\"Precision: \",precision_score(y_test, y_pred))\n",
    "#print(\"Recall: \", recall_score(y_test, y_pred))    \n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))   \n",
    "print(\"Performance overall: \")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "import joblib\n",
    "joblib.dump(grid.best_estimator_, 'model name', compress = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    print(\"Classes before:\", classes)\n",
    "    print(\"unique labels:\", unique_labels(y_true, y_pred))\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    print(\"Classes:\", classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "class_names = np.array([\"M\", \"F\", \"U\"])\n",
    "#class_names = np.array([\"M\", \"F\"])\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append columns for gender and type of diabetes prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    tweet = prep.replace_hashtags_URL_USER(tweet, mode_URL=\"replace\", mode_Mentions=\"replace\")\n",
    "    tweet = prep.tokenize(tweet)\n",
    "    return tweet\n",
    "\n",
    "def userName_to_vec(name):\n",
    "    try:\n",
    "        firstName = unicodedata.normalize('NFKD', name).encode('ascii', 'ignore').decode('utf-8', 'ignore').split(\" \")[0].replace(\" \", \"\")\n",
    "        vec = model_we[firstName]\n",
    "    except:\n",
    "        vec = np.zeros((model_we.vector_size, ))\n",
    "    return vec\n",
    "df = pd.read_csv(r\"\") #import dataset \n",
    "#del df[\"emotion\"]\n",
    "#del df[\"__index_level_1__\"]\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the gender classifier to data\n",
    "temp = pd.DataFrame()\n",
    "temp[\"text\"] = df.text.map(lambda tweet: tweet_vectorizer(preprocess_tweet(tweet), model_we))\n",
    "temp[\"user_description\"] = df.user_description.map(lambda userDesc: np.zeros((model_we.vector_size, )) \n",
    "                                                if isinstance(userDesc, float) or userDesc == \" \" or userDesc == None\n",
    "                                                else tweet_vectorizer(preprocess_tweet(userDesc), model_we))\n",
    "temp[\"user_name\"] = df.user_name.map(lambda name: userName_to_vec(name))\n",
    "\n",
    "gender_classifier = joblib.load(\"\")\n",
    "\n",
    "df[\"gender\"] = gender_classifier.predict(temp)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export new dataset as csv\n",
    "df.to_csv(r\"\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
