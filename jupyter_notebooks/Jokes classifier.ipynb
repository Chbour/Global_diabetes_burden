{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ac994b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have launched this NoteBook on Google Collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a09a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install nlp\n",
    "!pip install sentencepiece\n",
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcdfb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from google.colab import drive, files\n",
    "import pandas as pd\n",
    "import gspread\n",
    "from gspread_dataframe import get_as_dataframe, set_with_dataframe\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()  # verify your account to read files which you have access to. Make sure you have permission to read the file!\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "#gc = gspread.authorize(GoogleCredentials.get_application_default()) \n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9532a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation for BertTweet\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from emoji import demojize\n",
    "import re\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "# https://huggingface.co/vinai/bertweet-base\n",
    "def normalizeToken(token):\n",
    "    lowercased_token = token.lower()\n",
    "    if token.startswith(\"@\"):\n",
    "        return \"@USER\"\n",
    "    elif lowercased_token.startswith(\"http\") or lowercased_token.startswith(\"www\"):\n",
    "        return \"HTTPURL\"\n",
    "    elif len(token) == 1:\n",
    "        return demojize(token)\n",
    "    else:\n",
    "        if token == \"’\":\n",
    "            return \"'\"\n",
    "        elif token == \"…\":\n",
    "            return \"...\"\n",
    "        else:\n",
    "            return token\n",
    "\n",
    "def normalizeTweet(tweet):\n",
    "\n",
    "    tokens = tokenizer.tokenize(tweet.replace(\"’\", \"'\").replace(\"…\", \"...\"))\n",
    "    normTweet = \" \".join([normalizeToken(token) for token in tokens])\n",
    "\n",
    "    normTweet = normTweet.replace(\"cannot \", \"can not \").replace(\"n't \", \" n't \").replace(\"n 't \", \" n't \").replace(\"ca n't\", \"can't\").replace(\"ai n't\", \"ain't\")\n",
    "    normTweet = normTweet.replace(\"'m \", \" 'm \").replace(\"'re \", \" 're \").replace(\"'s \", \" 's \").replace(\"'ll \", \" 'll \").replace(\"'d \", \" 'd \").replace(\"'ve \", \" 've \")\n",
    "    normTweet = normTweet.replace(\" p . m .\", \"  p.m.\") .replace(\" p . m \", \" p.m \").replace(\" a . m .\", \" a.m.\").replace(\" a . m \", \" a.m \")\n",
    "\n",
    "    normTweet = re.sub(r\",([0-9]{2,4}) , ([0-9]{2,4})\", r\",\\1,\\2\", normTweet)\n",
    "    normTweet = re.sub(r\"([0-9]{1,3}) / ([0-9]{2,4})\", r\"\\1/\\2\", normTweet)\n",
    "    normTweet = re.sub(r\"([0-9]{1,3})- ([0-9]{2,4})\", r\"\\1-\\2\", normTweet)\n",
    "    \n",
    "    return \" \".join(normTweet.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload file \n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd056b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read uploaded file\n",
    "tweets = pd.read_csv(\"file_name\")\n",
    "tweets=tweets.rename(columns={\"fulltext\": \"text\"})\n",
    "print(tweets.shape)\n",
    "tweets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83247f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class TweetDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        for key in self.encodings.keys():\n",
    "            return len(self.encodings[key])\n",
    "        #return len(self.labels)\n",
    "\n",
    "def proba_to_category(row):\n",
    "  #print(row)\n",
    "  score_0, score_1 = row.iloc[0], row.iloc[1]\n",
    "\n",
    "  if score_0 < 0.5 and score_1 >= 0.5:\n",
    "    return 1\n",
    "  else: return 0\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device: {}\".format(device))\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"path_model\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7f19c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncation, padding = true ensures that all sentences are padded to the same length and are truncated to be no longer model's max input lengts\n",
    "# => allows to feed batches of sequences \n",
    "tweets_encodings = tokenizer(tweets.text.map(normalizeTweet).values.tolist(), truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "tweetDataSet = TweetDataSet(tweets_encodings)\n",
    "\n",
    "model.eval()\n",
    "tweetsLoader = DataLoader(tweetDataSet, batch_size=16)\n",
    "print(\"len tweetsLoader: {}\".format(len(tweetsLoader)))\n",
    "\n",
    "predicted = pd.Series()\n",
    "for (i, batch) in enumerate(tweetsLoader): \n",
    "    if i % 2000 == 0 : print(i)\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    proba = F.softmax(outputs[0]).detach().cpu().numpy()  # get probabilities from output\n",
    "    predicted_labels = pd.DataFrame(proba).apply(proba_to_category, axis=1) # get predicted class (highest proba)\n",
    "    predicted = predicted.append(predicted_labels, ignore_index=True)\n",
    "\n",
    "print(\"predicted: {}\".format(predicted.shape))\n",
    "print(predicted.value_counts())\n",
    "\n",
    "tweets[\"jokes\"] = predicted.values\n",
    "tweets_no_jokes = tweets[tweets[\"jokes\"] == 0]\n",
    "print(\"No jokes tweets: {}\".format(tweets_no_jokes.shape))\n",
    "\n",
    "del tweets_no_jokes[\"jokes\"]\n",
    "tweets_no_jokes.to_csv(\"saving_path\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
