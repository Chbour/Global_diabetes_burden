{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path as op\n",
    "import unicodedata\n",
    "import sys\n",
    "from gensim.models import FastText\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_predict, StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import classification_report, make_scorer\n",
    "\n",
    "basename = r\"..\" #path to basename file\n",
    "path_utils = op.join(basename , \"utils\")\n",
    "sys.path.insert(0, path_utils)\n",
    "\n",
    "from sys_utils import load_library\n",
    "from tweet_utils import *\n",
    "\n",
    "from preprocess import Preprocess\n",
    "prep = Preprocess()\n",
    "\n",
    "model_we = FastText.load(r\"fasttext_path\")\n",
    "\n",
    "trainingData = pd.read_csv(r\"path_labelled_data\")\n",
    "\n",
    "\n",
    "print(trainingData.dtypes)\n",
    "trainingData.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    tweet = prep.replace_hashtags_URL_USER(tweet, mode_URL=\"replace\", mode_Mentions=\"replace\")\n",
    "    tweet = prep.tokenize(tweet)\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def create_history_typeDiabetes_column(row):\n",
    "    \"\"\" Create column history of type diabetes \"\"\"\n",
    "    if row[\"History_TypeDiab\"] == 0: return 0\n",
    "    elif row[\"History_TypeDiab\"] == 1: return 1\n",
    "    elif row[\"History_TypeDiab\"] == 2: return 2\n",
    "    elif pd.isnull(row[\"History_TypeDiab\"]): return row[\"Type_Diabetes\"]\n",
    "    else: print(\"ERROR: Should not occur:  \", row[\"Type_Diabetes\"], \";;;\", row[\"Type_Diabetes\"])\n",
    "\n",
    "trainingData['history_typeDiab_total'] = trainingData.apply (lambda row: create_history_typeDiabetes_column(row), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label = \"Type_Diabetes\"\n",
    "label = \"history_typeDiab_total\"\n",
    "data_pd = trainingData[[\"text\", \"user_description\", \"user_name\", label]][:30000]\n",
    "\n",
    "data_pd.head()\n",
    "\n",
    "data_pd.text = data_pd.text.map(lambda tweet: tweet_vectorizer(preprocess_tweet(tweet), model_we))\n",
    "data_pd.user_description = data_pd.user_description.map(lambda userDesc: np.zeros((model_we.vector_size, )) \n",
    "                                                if isinstance(userDesc, float) or userDesc == \" \" \n",
    "                                                else tweet_vectorizer(preprocess_tweet(userDesc), model_we))\n",
    "\n",
    "\n",
    "# remove the tweets that are empty because there is no word embedding\n",
    "data_pd = data_pd[data_pd[\"text\"].apply(lambda x: len(x)>0) ]\n",
    "print(data_pd.shape)\n",
    "\n",
    "#data_pd.user_name = data_pd.user_name.map(lambda tweet: prep.remove_non_ascii(tweet))\n",
    "data_pd.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemSelect(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data):\n",
    "        return np.asarray(data[self.key].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose algo:\n",
    "#---------------------------------------------------------------------------\n",
    "modelAlgo = \"SVC\"\n",
    "\n",
    "if modelAlgo == \"MultinomialNB\":\n",
    "    model = MultinomialNB(random_state=0)\n",
    "elif modelAlgo == \"SVC\":\n",
    "    model = SVC(random_state=0)\n",
    "elif modelAlgo == \"logReg\":\n",
    "    model = LogisticRegression(random_state=0)\n",
    "elif modelAlgo == \"RandomForest\" :\n",
    "    model = RandomForestClassifier(random_state=0)\n",
    "elif modelAlgo == \"XGBoost\" :\n",
    "    model = XGBClassifier(random_state=0)\n",
    "elif modelAlgo == \"MLP\" :\n",
    "    model = MLPClassifier(early_stopping=True, batch_size=32, random_state=0)\n",
    "\n",
    "from imblearn.pipeline import Pipeline    \n",
    "    \n",
    "pipeline  = Pipeline([\n",
    "                ('union', FeatureUnion(\n",
    "                            transformer_list = [\n",
    "                                ('tweet', Pipeline([\n",
    "                                    ('tweetsSelector', ItemSelect(key='text')),\n",
    "                                ])),  \n",
    "                                ('userDesc', Pipeline([\n",
    "                                    ('userDescSelector', ItemSelect(key='user_description'))\n",
    "                                ])),\n",
    "                            ],\n",
    "                )),\n",
    "                ('smote', SMOTE(random_state=12, sampling_strategy=\"auto\", n_jobs=-1)), # , ratio = 1.0\n",
    "                ('model', model),\n",
    "            ])\n",
    "\n",
    "\n",
    "# parameter grid for grid search by using fastText embeddings\n",
    "parameters = {\n",
    "                'union__transformer_weights' : #[#{\"tweet\": 1, \"userDesc\":1, \"userName\":1},\n",
    "#                                                {\"tweet\": 1, \"userDesc\":1, \"userName\":0.8}, \n",
    "#                                                {\"tweet\": 1, \"userDesc\":1, \"userName\":0.5},\n",
    "#                                                {\"tweet\": 1, \"userDesc\":1, \"userName\":0.5},\n",
    "#                                                {\"tweet\": 1, \"userDesc\":1, \"userName\":0.4},\n",
    "#                                                {\"tweet\": 1, \"userDesc\":0.8, \"userName\":0.4},\n",
    "#                                                {\"tweet\": 1, \"userDesc\":0.8, \"userName\":0.5},\n",
    "#                                                {\"tweet\": 1, \"userDesc\":0.8, \"userName\":0.6},\n",
    "#                                                {\"tweet\": 1, \"userDesc\":0.7, \"userName\":0.5},\n",
    "#                                                {\"tweet\": 1, \"userDesc\":0.9, \"userName\":0.5},\n",
    "#                                                ],\n",
    "                                               [#{\"tweet\": 1, \"userDesc\":1}, \n",
    "#                                                {\"tweet\": 1, \"userDesc\":0.7}, \n",
    "#                                                {\"tweet\": 1, \"userDesc\":0.5},\n",
    "#                                                {\"tweet\": 1, \"userDesc\":0.3},\n",
    "#                                                {\"tweet\": 1, \"userDesc\":0.1},\n",
    "#                                                {\"tweet\": 1, \"userDesc\":0.0},\n",
    "#                                                {\"tweet\": 0, \"userDesc\":1}\n",
    "                                                ],\n",
    "    \n",
    "#               'smote__k_neighbors' : [3],\n",
    "#               # param for SVC\n",
    "#               'model__kernel' : [\"linear\"],#[\"linear\", \"poly\", \"rbf\"],\n",
    "#               'model__C' : [0.5],\n",
    "#               'model__tol' : [1e-2],\n",
    "#               'model__class_weight' : [\"balanced\", {0:1, 1:1, 2:1}, {0:1, 1:2, 2:1}, {0:1, 1:1, 2:2}, {0:1, 1:2, 2:2}],\n",
    "#\n",
    "#               # param for RandomForestClassifier\n",
    "#               'model__n_estimators' : [50, 100, 150],\n",
    "#               'model__criterion' : ['gini', 'entropy'],\n",
    "#               'model__max_features' : ['auto', 'log2'],\n",
    "#               'model__max_depth' : [ 5, 10, 20, 30]\n",
    "#\n",
    "#               # param for XGBoost Best: 0.910828 using {'model__learning_rate': 0.05, 'model__reg_alpha': 0, 'model__max_depth': 3, 'model__reg_lambda': 1.5, 'model__n_estimators': 300}\n",
    "#               'model__max_depth' : [3,4],\n",
    "#               'model__learning_rate' : [0.5, 0.1, 0.05],#, 0.01, 0.001],\n",
    "#               'model__booster' : [\"gblinear\"], #[\"gbtree\", \"gblinear\", \"dart\"],\n",
    "#               'model__gamma' : [0, 0.01],\n",
    "#               'model__n_estimators' : [80, 100, 150],\n",
    "#               'model__reg_alpha' : [0, 0.1],\n",
    "#               'model__reg_lambda' : [0.5, 1.0]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(sex):\n",
    "    # no type 0, type 1 = 1, type 2 = 2\n",
    "    if sex == 0: return(-1) \n",
    "    else: return(1)\n",
    "    \n",
    "    \n",
    "print(\"data before filter out gestational diabetes:\", data_pd.shape, type(data_pd))\n",
    "data_pd_withoutGestational = data_pd.loc[data_pd[label] != 3] # ignore tweets with gestational diabetes as there are too few\n",
    "\n",
    "#print(\"data after before filter out gestational diabetes:\", data_pd_withoutGestational.shape, type(data_pd_withoutGestational))\n",
    "X = data_pd_withoutGestational[[\"text\", \"user_description\"]]\n",
    "y = data_pd_withoutGestational[label]\n",
    "\n",
    "print(\"X :\", X.shape, type(X))\n",
    "print(\"y.unique: \", y.unique())\n",
    "print(y.value_counts())\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) \n",
    "\n",
    "X_train_pd = pd.DataFrame(X_train, columns=[\"text\", \"user_description\"])\n",
    "X_test_pd = pd.DataFrame(X_test, columns=[\"text\", \"user_description\"])\n",
    "\n",
    "#from sklearn.metrics import precision_score, roc_auc_score, make_scorer\n",
    "prec_scorer = make_scorer(precision_score, average=\"micro\")\n",
    "print(\"Start Grid search...\")\n",
    "grid = GridSearchCV(pipeline, parameters, cv=10, n_jobs=-2, verbose=2, scoring=prec_scorer)\n",
    "grid.fit(X_train_pd, y_train)\n",
    "print(\"\\nBest: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
    "\n",
    "y_pred = grid.best_estimator_.predict(X_test_pd)\n",
    "#print(\"F1-Score:\", f1_score(y_test, y_pred))\n",
    "#print(\"Precision: \",precision_score(y_test, y_pred))\n",
    "#print(\"Recall: \", recall_score(y_test, y_pred))    \n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))   \n",
    "print(\"Performance overall: \")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC - SMOTE - precision scoring\n",
    " \n",
    "\n",
    "# Take this one\n",
    "\n",
    "# SVC - no SMOTE\n",
    "#Best: 0.733737 using {'model__C': 0.5, 'model__kernel': 'linear', 'model__tol': 0.1, 'union__transformer_weights': {'tweet': 1, 'userDesc': 0.5}}\n",
    "#Accuracy:  0.7530864197530864\n",
    "#Performance overall: \n",
    "#              precision    recall  f1-score   support\n",
    "\n",
    "#           0       0.71      0.87      0.78       243\n",
    "#           1       0.83      0.69      0.75       157\n",
    "#           2       0.78      0.64      0.70       167\n",
    "\n",
    "#   micro avg       0.75      0.75      0.75       567\n",
    "#   macro avg       0.77      0.73      0.75       567\n",
    "#weighted avg       0.76      0.75      0.75       567\n",
    "\n",
    "\n",
    "# SVC - SMOTE\n",
    "#Best: 0.739032 using {'model__C': 0.5, 'model__kernel': 'linear', 'model__tol': 0.01, 'smote__k_neighbors': 3, 'union__transformer_weights': {'tweet': 1, 'userDesc': 0.3}}\n",
    "#Accuracy:  0.7372134038800705\n",
    "#Performance overall: \n",
    "#              precision    recall  f1-score   support\n",
    "\n",
    "#           0       0.74      0.78      0.76       243\n",
    "#           1       0.72      0.69      0.71       157\n",
    "#           2       0.75      0.72      0.73       167\n",
    "\n",
    "#   micro avg       0.74      0.74      0.74       567\n",
    "#   macro avg       0.74      0.73      0.73       567\n",
    "#weighted avg       0.74      0.74      0.74       567\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "import joblib\n",
    "joblib.dump(grid.best_estimator_, '', compress = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    print(\"Classes before:\", classes)\n",
    "    print(\"unique labels:\", unique_labels(y_true, y_pred))\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    print(\"Classes:\", classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "#class_names = np.array([\"M\", \"F\", \"U\"])\n",
    "class_names = np.array([\"Unknown\", \"Type 1\", \"Type 2\"])\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict having diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    tweet = prep.replace_hashtags_URL_USER(tweet, mode_URL=\"replace\", mode_Mentions=\"replace\")\n",
    "    tweet = prep.tokenize(tweet)\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def create_haveDiab_column(row):\n",
    "    if isinstance(row[\"History_HasDiab\"], str): return row[\"HasDiabetes\"]\n",
    "    elif float(row[\"History_HasDiab\"]) < 1e-9: return 0\n",
    "    elif float(row[\"History_HasDiab\"])-1 < 1e-9: return 1\n",
    "    elif pd.isnull(row[\"History_HasDiab\"]) : return row[\"HasDiabetes\"]\n",
    "    else: print(\"ERROR: Should not occur:  \", row[\"HasDiabetes\"], \";;;\", row[\"History_HasDiab\"])\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = prep.replace_hashtags_URL_USER(tweet, mode_URL=\"replace\", mode_Mentions=\"replace\")\n",
    "    tweet = prep.tokenize(tweet)\n",
    "    return tweet\n",
    "\n",
    "def userName_to_vec(name):\n",
    "    try:\n",
    "        firstName = unicodedata.normalize('NFKD', name).encode('ascii', 'ignore').decode('utf-8', 'ignore').split(\" \")[0].replace(\" \", \"\")\n",
    "        vec = model_we[firstName]\n",
    "    except:\n",
    "        vec = np.zeros((model_we.vector_size, ))\n",
    "    return vec\n",
    "df = pd.read_csv(r\"\") #import dataset \n",
    "#del df[\"emotion\"]\n",
    "#del df[\"__index_level_1__\"]\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "print(df.head())\n",
    "\n",
    "df['HaveDiab_merge'] = df.apply (lambda row: create_haveDiab_column(row), axis=1)\n",
    "\n",
    "df.HaveDiab_merge.value_counts()\n",
    "\n",
    "df = pd.read_csv(r\"\") #import dataset \n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"Type_Diabetes\"\n",
    "data_pd = df[[\"text\", \"user_description\", \"user_name\", label]]\n",
    "\n",
    "temp = pd.DataFrame()\n",
    "temp[\"text\"] = df.text.map(lambda tweet: tweet_vectorizer(preprocess_tweet(tweet), model_we))\n",
    "temp[\"user_description\"] = df.user_description.map(lambda userDesc: np.zeros((model_we.vector_size, )) \n",
    "                                                if isinstance(userDesc, float) or userDesc == \" \" or userDesc == None\n",
    "                                                else tweet_vectorizer(preprocess_tweet(userDesc), model_we))\n",
    "temp[\"user_name\"] = df.user_name.map(lambda name: userName_to_vec(name))\n",
    "\n",
    "gender_classifier = joblib.load(\"\")\n",
    "\n",
    "df[\"Type_diabetes\"] = gender_classifier.predict(temp)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export new dataset as csv\n",
    "df.to_csv(r\"\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
