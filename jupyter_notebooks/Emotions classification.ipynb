{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7fc270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re \n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_predict, StratifiedShuffleSplit, train_test_split\n",
    "import os.path as op\n",
    "import sys\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892616ab",
   "metadata": {},
   "source": [
    "## Transform labels into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c51538",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(r\"\") #Import datasets with labels according to emotions\n",
    "data=data.rename(columns={\"text\": \"Text\", \"emotion\": \"Emotion\"})\n",
    "data=data.drop([\"Unnamed: 0\",\"Clean_Text\"],axis=1)\n",
    "data[\"vec_emo\"]=\"\"\n",
    "for i in tqdm(range(0,len(data.index))): #possibility to use up to 7 emotions\n",
    "    if data[\"Emotion\"][i]==\"joy\":\n",
    "        data[\"vec_emo\"][i]=\"[ 1.  0.  0.  0.  0.  0.  0.]\"\n",
    "    elif data[\"Emotion\"][i]==\"fear\":\n",
    "        data[\"vec_emo\"][i]=\"[ 0.  1.  0.  0.  0.  0.  0.]\"\n",
    "    elif data[\"Emotion\"][i]==\"anger\":\n",
    "        data[\"vec_emo\"][i]=\"[ 0.  0.  1.  0.  0.  0.  0.]\"\n",
    "    elif data[\"Emotion\"][i]==\"sadness\":\n",
    "        data[\"vec_emo\"][i]=\"[ 0.  0.  0.  1.  0.  0.  0.]\"\n",
    "    elif data[\"Emotion\"][i]==\"disgust\":\n",
    "        data[\"vec_emo\"][i]=\"[ 0.  0.  0.  0.  1.  0.  0.]\"\n",
    "    elif data[\"Emotion\"][i]==\"shame\":\n",
    "        data[\"vec_emo\"][i]=\"[ 0.  0.  0.  0.  0.  1.  0.]\"\n",
    "    elif data[\"Emotion\"][i]==\"guilt\":\n",
    "        data[\"vec_emo\"][i]=\"[ 0.  0.  0.  0.  0.  0.  1.]\"\n",
    "data=data.drop([\"Emotion\"],axis=1)\n",
    "data=data[[\"vec_emo\",\"Text\"]]\n",
    "data.to_csv(r'export_data_in_txt_file', header=None, index=None, sep='\\t', mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7478476",
   "metadata": {},
   "source": [
    "## Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50585671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dataset and preprocess the text, scripts from \n",
    "\n",
    "basename = r\"\" #connect to basename\n",
    "path_utils = op.join(basename , \"utils\")\n",
    "sys.path.insert(0, path_utils)\n",
    "\n",
    "from sys_utils import load_library\n",
    "from tweet_utils import *\n",
    "\n",
    "from preprocess import Preprocess\n",
    "prep = Preprocess()\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = prep.replace_contractions(tweet)\n",
    "    tweet = prep.replace_hashtags_URL_USER(tweet, mode_URL=\"delete\", mode_Mentions=\"delete\")\n",
    "    tweet = prep.remove_repeating_characters(tweet)\n",
    "    tweet = prep.remove_repeating_words(tweet)\n",
    "    tweet = prep.tokenize(tweet)\n",
    "    tweet = prep.to_lowercase(tweet)\n",
    "    tweet = prep.remove_non_ascii(tweet)\n",
    "    tweet = prep.replace_numbers(tweet)\n",
    "    tweet = \" \".join([word for word in tweet])\n",
    "    return tweet\n",
    "\n",
    "def read_data(file):\n",
    "    data = []\n",
    "    with open(file, 'r',encoding=\"utf8\")as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            label = ' '.join(line[1:line.find(\"]\")].strip().split())\n",
    "            text = line[line.find(\"]\")+1:].strip()\n",
    "            data.append([label, text])\n",
    "    return data\n",
    "\n",
    "file = '' #text file name\n",
    "df = read_data(file)\n",
    "print(\"Number of instances: {}\".format(len(df)))\n",
    "\n",
    "#Preprocess\n",
    "for i in tqdm(range(len(df))):\n",
    "    df[i][1]=preprocess_tweet(df[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ff0016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram(token, n): \n",
    "    output = []\n",
    "    for i in range(n-1, len(token)): \n",
    "        ngram = ' '.join(token[i-n+1:i+1])\n",
    "        output.append(ngram) \n",
    "    return output\n",
    "\n",
    "def create_feature(text, nrange=(1, 1)):\n",
    "    text_features = [] \n",
    "    text = text.lower() \n",
    "    text_alphanum = re.sub('[^a-z0-9#]', ' ', text)\n",
    "    for n in range(nrange[0], nrange[1]+1): \n",
    "        text_features += ngram(text_alphanum.split(), n)    \n",
    "    text_punc = re.sub('[a-z0-9]', ' ', text)\n",
    "    text_features += ngram(text_punc.split(), 1)\n",
    "    return Counter(text_features)\n",
    "\n",
    "def convert_label(item, name): \n",
    "    items = list(map(float, item.split()))\n",
    "    label = \"\"\n",
    "    for idx in range(len(items)): \n",
    "        if items[idx] == 1: \n",
    "            label += name[idx] + \" \"\n",
    "    \n",
    "    return label.strip()\n",
    "\n",
    "emotions = [\"joy\", 'fear', \"anger\", \"sadness\",\"disgust\", \"shame\", \"guilt\"]\n",
    "\n",
    "\n",
    "X_all = []\n",
    "y_all = []\n",
    "for label, text in df:\n",
    "    y_all.append(convert_label(label, emotions))\n",
    "    X_all.append(create_feature(text, nrange=(1, 4)))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size = 0.2, random_state = 150)\n",
    "\n",
    "\n",
    "def train_test(clf, X_train, X_test, y_train, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_acc = accuracy_score(y_train, clf.predict(X_train))\n",
    "    test_acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "    return train_acc, test_acc\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vectorizer = DictVectorizer(sparse = True)\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "svc = SVC(random_state=32)\n",
    "svm = LinearSVC(tol=1e-1,random_state=32,max_iter=10000)\n",
    "lsvc = CalibratedClassifierCV(svm)\n",
    "rforest = RandomForestClassifier(random_state=32)\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "clifs = [lsvc] #svm, svc, rforest, dtree\n",
    "\n",
    "print(\"| {:25} | {} | {} |\".format(\"Classifier\", \"Training Accuracy\", \"Test Accuracy\"))\n",
    "print(\"| {} | {} | {} |\".format(\"-\"*25, \"-\"*17, \"-\"*13))\n",
    "for clf in clifs: \n",
    "    clf_name = clf.__class__.__name__\n",
    "    train_acc, test_acc = train_test(clf, X_train, X_test, y_train, y_test)\n",
    "    print(\"| {:25} | {:17.7f} | {:13.7f} |\".format(clf_name, train_acc, test_acc))\n",
    "    \n",
    "print(\"-----Saving model-----\")\n",
    "import joblib\n",
    "pipeline_file = open(\"file to save the model, save as .pkl\",\"wb\")\n",
    "joblib.dump(clf,pipeline_file)\n",
    "pipeline_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f78618c",
   "metadata": {},
   "source": [
    "## Apply model to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf014b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import model and data\n",
    "loaded_model = joblib.load(r\"\") #load model\n",
    "\n",
    "basename = r\"\" #path to basename\n",
    "path_utils = op.join(basename , \"utils\")\n",
    "sys.path.insert(0, path_utils)\n",
    "\n",
    "from sys_utils import load_library\n",
    "from tweet_utils import *\n",
    "\n",
    "from preprocess import Preprocess\n",
    "prep = Preprocess()\n",
    "\n",
    "data=pd.read_csv(r\"\",usecols=[\"id\",\"text\"]) #import data to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66ff502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    tweet = prep.replace_contractions(tweet)\n",
    "    tweet = prep.replace_hashtags_URL_USER(tweet, mode_URL=\"delete\", mode_Mentions=\"delete\")\n",
    "    tweet = prep.remove_repeating_characters(tweet)\n",
    "    tweet = prep.remove_repeating_words(tweet)\n",
    "    tweet = prep.tokenize(tweet)\n",
    "    tweet = prep.to_lowercase(tweet)\n",
    "    tweet = prep.remove_non_ascii(tweet)\n",
    "    tweet = prep.replace_numbers(tweet)\n",
    "    tweet = \" \".join([word for word in tweet])\n",
    "    return tweet\n",
    "\n",
    "data[\"clean_text\"] = data.text.apply(preprocess_tweet) #preprocess tweet\n",
    "\n",
    "def ngram(token, n): \n",
    "    output = []\n",
    "    for i in range(n-1, len(token)): \n",
    "        ngram = ' '.join(token[i-n+1:i+1])\n",
    "        output.append(ngram) \n",
    "    return output\n",
    "\n",
    "def create_feature(text, nrange=(1, 1)):\n",
    "    text_features = [] \n",
    "    text = text.lower() \n",
    "    text_alphanum = re.sub('[^a-z0-9#]', ' ', text)\n",
    "    for n in range(nrange[0], nrange[1]+1): \n",
    "        text_features += ngram(text_alphanum.split(), n)    \n",
    "    text_punc = re.sub('[a-z0-9]', ' ', text)\n",
    "    text_features += ngram(text_punc.split(), 1)\n",
    "    return Counter(text_features)\n",
    "\n",
    "def convert_label(item, name): \n",
    "    items = list(map(float, item.split()))\n",
    "    label = \"\"\n",
    "    for idx in range(len(items)): \n",
    "        if items[idx] == 1: \n",
    "            label += name[idx] + \" \"\n",
    "    \n",
    "    return label.strip()\n",
    "\n",
    "emotions = [\"joy\", 'fear', \"anger\", \"sadness\", \"disgust\" , \"shame\", \"guilt\"]\n",
    "\n",
    "\n",
    "X_all = []\n",
    "y_all = []\n",
    "for text in data[\"clean_text\"].to_list():\n",
    "    X_all.append(create_feature(text, nrange=(1,4)))\n",
    "    \n",
    "X_class = vectorizer.transform(X_all)\n",
    "\n",
    "#predict classes\n",
    "probas_predicted=loaded_model.predict_proba(X_class)\n",
    "class_predicted=loaded_model.predict(X_class)\n",
    "\n",
    "data[\"proba_emotion\"]=\"\"\n",
    "data[\"emotion\"]=\"\"\n",
    "for i in tqdm(range(len(data.index))):\n",
    "    data[\"proba_emotion\"][i]=list(probas_predicted[i])\n",
    "    data[\"emotion\"][i]=class_predicted[i]\n",
    "    \n",
    "#export dataset with emotions\n",
    "data.to_csv(r\"path\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
