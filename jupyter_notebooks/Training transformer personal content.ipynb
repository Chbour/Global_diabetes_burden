{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have launched this NoteBook on Google Collab and used https://huggingface.co/transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install nlp\n",
    "!pip install sentencepiece\n",
    "!pip install emoji\n",
    "!pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from google.colab import drive, files\n",
    "import pandas as pd\n",
    "import gspread\n",
    "from gspread_dataframe import get_as_dataframe, set_with_dataframe\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()  # verify your account to read files which you have access to. Make sure you have permission to read the file!\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "\n",
    "gc = gspread.authorize(GoogleCredentials.get_application_default()) \n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload file\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read imported file\n",
    "import pandas as pd\n",
    "print(uploaded)\n",
    "df = pd.read_csv(io.BytesIO(uploaded['']))\n",
    "print(df.shape)\n",
    "print(df[\"personal\"].value_counts())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mount in a folder of google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example on a tweet, scripts from Huggingface\n",
    "# Normalisation for BertTweet\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from emoji import demojize\n",
    "import re\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "# https://huggingface.co/vinai/bertweet-base\n",
    "def normalizeToken(token):\n",
    "    lowercased_token = token.lower()\n",
    "    if token.startswith(\"@\"):\n",
    "        return \"@USER\"\n",
    "    elif lowercased_token.startswith(\"http\") or lowercased_token.startswith(\"www\"):\n",
    "        return \"HTTPURL\"\n",
    "    elif len(token) == 1:\n",
    "        return demojize(token)\n",
    "    else:\n",
    "        if token == \"â€™\":\n",
    "            return \"'\"\n",
    "        elif token == \"â€¦\":\n",
    "            return \"...\"\n",
    "        else:\n",
    "            return token\n",
    "\n",
    "def normalizeTweet(tweet):\n",
    "\n",
    "    tokens = tokenizer.tokenize(tweet.replace(\"â€™\", \"'\").replace(\"â€¦\", \"...\"))\n",
    "    normTweet = \" \".join([normalizeToken(token) for token in tokens])\n",
    "\n",
    "    normTweet = normTweet.replace(\"cannot \", \"can not \").replace(\"n't \", \" n't \").replace(\"n 't \", \" n't \").replace(\"ca n't\", \"can't\").replace(\"ai n't\", \"ain't\")\n",
    "    normTweet = normTweet.replace(\"'m \", \" 'm \").replace(\"'re \", \" 're \").replace(\"'s \", \" 's \").replace(\"'ll \", \" 'll \").replace(\"'d \", \" 'd \").replace(\"'ve \", \" 've \")\n",
    "    normTweet = normTweet.replace(\" p . m .\", \"  p.m.\") .replace(\" p . m \", \" p.m \").replace(\" a . m .\", \" a.m.\").replace(\" a . m \", \" a.m \")\n",
    "\n",
    "    normTweet = re.sub(r\",([0-9]{2,4}) , ([0-9]{2,4})\", r\",\\1,\\2\", normTweet)\n",
    "    normTweet = re.sub(r\"([0-9]{1,3}) / ([0-9]{2,4})\", r\"\\1/\\2\", normTweet)\n",
    "    normTweet = re.sub(r\"([0-9]{1,3})- ([0-9]{2,4})\", r\"\\1-\\2\", normTweet)\n",
    "    \n",
    "    return \" \".join(normTweet.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.tokenize(normalizeTweet(df.iloc[0][\"text\"])))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from nlp import load_dataset\n",
    "from transformers import DistilBertTokenizerFast\n",
    "import torch\n",
    "\n",
    "data = df \n",
    "text = data[\"text\"].map(normalizeTweet).values.tolist()\n",
    "labels = data[\"personal\"].values.tolist()\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(text, labels, test_size=0.33)\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(text, labels, test_size=0.2)\n",
    "print(\"Train: {}\".format(len(train_texts)))\n",
    "print(\"Val: {}\".format(len(val_texts)))\n",
    "print(\"Test: {}\".format(len(test_texts)))\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
    "#tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# truncation, padding = true ensures that all sentences are padded to the same length and are truncated to be no longer model's max input lengts\n",
    "# => allows to feed batches of sequences \n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "print(len(train_encodings))\n",
    "print(len(test_encodings))\n",
    "print(len(val_encodings))\n",
    "print(val_encodings.keys())\n",
    "#model = BertForSequenceClassification.from_pretrained(\"vinai/bertweet-base\", num_labels=2) # create BERT Model with weights and randomly initializes sequence classification head with output size  2\n",
    "#model.train() # puts model in train mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform labels + encodings into Pytorch DataSet object (including __len__, __getitem__)\n",
    "class TweetDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      \n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = TweetDataSet(train_encodings, train_labels)\n",
    "val_dataset = TweetDataSet(val_encodings, val_labels)\n",
    "test_dataset = TweetDataSet(test_encodings, test_labels)\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine tune model\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=10,              # total number of training epochs\n",
    "    per_device_train_batch_size=64,  # batch size per device during training\n",
    "    per_device_eval_batch_size=32,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# AutoModelForSequenceClassification adds a fully connected layer after BERT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-base\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.device_count()) # 1 = connected to gpu\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate\n",
    "eval_output = trainer.evaluate(test_dataset) \n",
    "print(eval_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save fine-tuned model\n",
    "trainer.save_model(\"path\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
